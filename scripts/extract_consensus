#!/usr/bin/env python3

'''
input:  alignment file as BAM file
output: consensus sequences including either the majority base or the ambiguous
        bases as FASTA files
'''

import pysam
import argparse
import os
import numpy as np
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from smallgenomeutilities.__checkPath__ import CheckPath

def parse_args():
    """ Set up the parsing of command-line arguments """
    parser = argparse.ArgumentParser(
            description="Script to construct consensus sequences",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    requiredNamed = parser.add_argument_group('required named arguments')
    requiredNamed.add_argument("-i", required=True, metavar='BAM',
                               dest='bamfile', help="Input BAM file")
    parser.add_argument("-f", required=False, metavar='FASTA', dest='reference',
                        type=str,
                        help="Fasta file containing the reference sequence")
    parser.add_argument("-r", required=False, default=None, metavar='BED',
                        dest='region', type=str,
                        help="Region of interested in BED format, e.g. HXB2:2253-3869. Loci are interpreted using 0-based indexing, and a half-open interval is used, i.e, [start:end)")
    parser.add_argument("-c", required=False, default=50, metavar='INT',
                        dest='min_coverage', type=int,
                        help="Minimum read depth for reporting variants per locus")
    parser.add_argument("-q", required=False, default=15, metavar='INT',
                        dest='qual_thrd', type=int,
                        help="Minimum phred quality score a base has to reach to be counted")
    parser.add_argument("-a", required=False, default=0.05, metavar='FLOAT',
                        dest='min_freq', type=float, help="Minimum frequency for an ambiguous nucleotide")
    parser.add_argument("-n", required=False, default=None, metavar='INT',
                        dest='n_threshold', type=int,
                        help="Read count below which ambiguous base 'n' is reported")
    parser.add_argument("-N", required=False, default="CONSENSUS", type=str,
                        metavar='STR', dest="sampleID",
                        help="Patient/sample identifier")
    parser.add_argument("-o", required=False, default=os.getcwd(),
                        action=CheckPath, metavar='PATH', dest='outdir',
                        help="Output directory")

    return parser.parse_args()



def first_base_count(seq_count_tuple):
    '''Return the counts of unique first bases of an alignment column sequence count tuple possibly containing indels.

    Parameters:
        seq_count_tuple: tuple of the form ([bases/indels], [counts])
            The tuple contains a list of bases and (possibly) indels at this alignment column,
            and a list of the respective counts, as returned by np.unique(x, return_counts=True),
            where x is a column of a AlignmentFile.pileup()

    Returns:
        a tuple of the form ([bases], [counts]), where [bases] is a list of bases including the first bases
        of eventual indels and [counts] are their respective counts
    '''
    first_bases = [u[0] for u in seq_count_tuple[0]]
    first_bases_uniques = np.unique(first_bases, return_inverse=True)
    counts_collapsed = np.zeros_like(first_bases_uniques[0], dtype="int")
    for i in range(len(first_bases_uniques[1])):
        counts_collapsed[first_bases_uniques[1][i]] += seq_count_tuple[1][i]

    return (first_bases_uniques[0], counts_collapsed)

def build_insert_matrix(seq_count_tuple):
    '''Build an insert matrix from a sequence count tuple.

    Parameters:
        seq_count_tuple: tuple of the form ([bases/indels], [counts])
            The tuple contains a list of bases and (possibly) indels at this alignment column,
            and a list of the respective counts, as returned by np.unique(x, return_counts=True),
            where x is a column of a AlignmentFile.pileup()

    Returns:
        insert_matrix: np.array [n_inserts, max_insert_length]
            The insert matrix contains one row per insert and as many column as there are bases in the largest insert.
            The shorter inserts are right-padded with '*'.
    '''
    insert_matrix = []
    length = 0
    coverage = np.sum(np.sum(seq_count_tuple[1]))
    #make list of insertions at that position
    for j in range(len(seq_count_tuple[0])):
        if (len(seq_count_tuple[0][j])>2) and (seq_count_tuple[0][j][1] == '+'):
            # get number after + (this number can be 3,2 or 1 digit long)
            # this corresponds to the lenght of the insertion
            if (len(seq_count_tuple[0][j])>4) and (seq_count_tuple[0][j][4].isdigit()):
                curr_insert_len = int(seq_count_tuple[0][j][2:5])
                idx=5
            elif (len(seq_count_tuple[0][j])>3) and (seq_count_tuple[0][j][3].isdigit()):
                curr_insert_len = int(seq_count_tuple[0][j][2:4])
                idx=4
            elif seq_count_tuple[0][j][2].isdigit():
                curr_insert_len = int(seq_count_tuple[0][j][2:3])
                idx=3
            # find index where alphabet starts
            for n in range(seq_count_tuple[1][j]):
                insert_matrix.append(list(seq_count_tuple[0][j][idx:]))
            if length < curr_insert_len:
                length = curr_insert_len
    #right pad shorter insertions with "-"
    for m in range(len(insert_matrix)):
        while len(insert_matrix[m]) < length:
            insert_matrix[m].append('*') #replace here '*'->'-' if we arent supposed to count it as deletion

    insert_matrix = np.array(insert_matrix)
    insert_list = []

    if len(insert_matrix)>0: #e.g. if insert_matrix non-empty
        # go through columns of insert_matrix
        for c in range(insert_matrix.shape[1]):
            matrix_column = insert_matrix[:,c]
            if matrix_column.size > 0: # i.e. if we havent filtered out the whole column
                #keep most frequent
                unique = np.unique(matrix_column, return_counts=True)
                insert_list.append(unique)

    return insert_list



def collapse_insert_matrix(insert_matrix, min_coverage, min_freq,
                        n_insert_thres=2,ambiguous = False, authorised_alphabet=['A', 'T', 'G', 'C']):
    '''Function to build local consensus from insert matrix.

    Parameters:
        insert_matrix: np.array [n_inserts, max_insert_length]
            As returned by build_insert_matrix()
        authorised_alphabet: list of characters
            Characters in the insert_matrix that do not belong to authorised_alphabet will not be counted when
            building the local consensus.
        min_coverage: for #reads with insertions in that current column < min_coverage output lowercase letters
        n_insert_thres: if coverage < n_insert_thres then do not add insertion-column
        ambiguous: if True, compute ambiguous base at position
        min_freq: for computation of ambiguous base

    Returns:
        seq_out: list of characters
            A local consensus sequence built from the most frequent bases of each column of the insert matrix.

    '''
    seq_out = []

    for column in insert_matrix:
        index = np.in1d(column[0], authorised_alphabet)
        first_base = (column[0][index],
                      column[1][index])
        max_index = np.argmax(first_base[1])
        base = [first_base[0][max_index]]
        if ambiguous == True:
            base = ambiguous_bases(first_base,min_freq)
        if np.sum(first_base[1]) < min_coverage:
            # cast lowercase
            base= np.char.lower(base)
        if np.sum(first_base[1]) < n_insert_thres:
            return seq_out
        else:
            seq_out.extend(base)

    return seq_out


def check_coverage(first_base,base, min_coverage, n_threshold, curr_pos, reference = None,
                   reference_name = None, start=None, end=None):
    '''
    curr_pos: current position in the reference sequence
    If read depth is lower than min_coverage lowercase letters are reported
    Read count below n_threshold are reported as ambiguous base 'n'.
    If there is no n_threshold, base with zero count output 'n' or if reference available take value of reference.
    '''
    if np.sum(first_base[1])< min_coverage:
        # if read depth bigger than n_thres but still lower than min_coverage
        # cast lowercase
        base= np.char.lower(base)

    if (n_threshold is None):
        if (np.sum(first_base[1])==0):
            # if there are is no n_thres but we have zero counts at position
            # return 'n' or reference[curr_pos]
            if reference is None:
                base = ['n']
            else:
                reference = pysam.FastaFile(reference)
                assert reference.references[0] == reference_name, (
                    "the name of the genomic region and the reference differ")
                reference = reference.fetch(
                    reference=reference_name, start=start, end=end).lower()
                base = reference[curr_pos]

    elif np.sum(first_base[1]) < n_threshold:
        base = ['n']

    return base


def ambiguous_bases(first_base, min_freq):
    '''
    To report ambiguous bases, every base with a relative frequency greater or
    equal to min_freq is accounted for. But at least two reads should support
    that base.
    returns ambiguous base at current position
    '''
    coverage = np.sum(first_base[1])
    min_count = min_freq*coverage
    min_count = max(2, min_count)
    idx= np.where(first_base[1]>=min_count)

    # if len(idx) is zero, read counts are insufficient
    if len(first_base[0][idx])==0:
        return ['n']

    thres_majority = max(5, 0.75*coverage)
    idx_majority = np.where(first_base[1][idx]>=thres_majority)
    # if there is a base or deletion that has counts bigger than thres_ambig
    if len(first_base[0][idx][idx_majority])!= 0:
        max_idx = np.argmax(first_base[1][idx][idx_majority])
        return [first_base[0][idx][idx_majority][max_idx]]
    else:
        thres_ambiguous = coverage*0.25
        idx_ambiguous = np.where(first_base[1][idx]>=thres_ambiguous)
        # if any base in the set has read-count < 5 report 'N'
        idx_aux = np.where(first_base[1][idx][idx_ambiguous]<5)
        if len(first_base[1][idx][idx_ambiguous][idx_aux])>0:
            return ['N']
        elif '-' in set(first_base[0][idx][idx_ambiguous]):
            return ['N']
        elif len(first_base[1][idx][idx_ambiguous])==1:
            return ['N']
        else:
            ambiguous_set = first_base[0][idx][idx_ambiguous]
            # IUPAC nucleotide code form
            # https://www.bioinformatics.org/sms/iupac.html
            if set(ambiguous_set)==set(['A','G']):
                return ['R']
            if set(ambiguous_set)==set(['C','T']):
                return ['Y']
            if set(ambiguous_set)==set(['G','C']):
                return ['S']
            if set(ambiguous_set)==set(['A','T']):
                return ['W']
            if set(ambiguous_set)==set(['T','G']):
                return ['K']
            if set(ambiguous_set)==set(['A','C']):
                return ['M']
            if set(ambiguous_set)==set(['G','C','T']):
                return ['B']
            if set(ambiguous_set)==set(['G','A','T']):
                return ['D']
            if set(ambiguous_set)==set(['C','A','T']):
                return ['H']
            if set(ambiguous_set)==set(['G','A','C']):
                return ['V']
            if set(ambiguous_set)==set(['G','A','C','T']):
                return ['N']
            else:
                return ['n']


def main():
    # define alphabets for filtering sequences
    alphabet = np.array(['A', 'T', 'G', 'C'])
    alphabet_deletions = np.array(['A', 'T', 'G', 'C', '*'])
    alphabet_ambiguous = np.array(['n', 'T', 'G', 'K', 'C', 'Y', 'S', 'B', 'A', 'W', 'R', 'D', 'M', 'H', 'V', 'N'])
    alphabet_ambiguous_deletions = np.array(['n', 'T', 'G', 'K', 'C', 'Y', 'S', 'B', 'A', 'W', 'R', 'D', 'M', 'H', 'V', 'N', '*'])

    # parse arguments
    args = parse_args()

    reference_name = None
    if args.region is not None:
        aux = args.region.split(":")
        reference_name = aux[0]
        sampleID = ':'.join((args.sampleID, aux[1]))
        aux = aux[1].split('-')
        start = int(aux[0])
        end = int(aux[1])
    else:
        start = None
        end = None
        sampleID = args.sampleID


    # 1. Load BAM file and get base/count tuples per loci:
    quer_sequences_count = []
    reference_pos = []
    with pysam.AlignmentFile(args.bamfile, 'rb') as alnfile:
        if reference_name is None:
            reference_name = alnfile.references[0]
        reference_length = alnfile.get_reference_length(reference_name)
        # make list of ([bases], [counts]) tuples at each pileup column
        pileup = alnfile.pileup(
            contig=reference_name, start=start, stop=end,
            min_base_quality=args.qual_thrd)
        for pos in pileup:
            pileup_column = pos.get_query_sequences(add_indels=True)
            pileup_column = [item.upper() for item in pileup_column]
            count_tuple = np.unique(pileup_column, return_counts=True)
            reference_pos.append(pos.reference_pos)
            quer_sequences_count.append(count_tuple)

    # 2. Build consensus sequence

    # lists to contain consensus sequences
    cons_majority = [] # holds consensus sequence without deletions and without ambiguous bases
    cons_majority_deletions = [] # holds consensus sequence with deletions and without ambiguous bases
    cons_majority_ambiguous = [] # holds consensus sequence without deletions and with ambiguous bases
    cons_majority_ambiguous_deletions = [] # holds consensus sequence with deletions and with ambiguous bases

    # iterate through the alignment columns counts and make local consensus for each output type
    for idx, column in enumerate(quer_sequences_count):
        # get the counts for only the first base
        first_base_ambiguous_deletions = first_base_count(column)
        # build the insert matrix
        insert_matrix = build_insert_matrix(column)


        # 2.1 make consensus at column without deletions and without ambiguous bases
        index = np.in1d(first_base_ambiguous_deletions[0], alphabet)
        first_base = (first_base_ambiguous_deletions[0][index],
                      first_base_ambiguous_deletions[1][index])
        if first_base[0].size > 0: # i.e. if we havent filtered out the whole column
            max_index = np.argmax(first_base[1])
            base = [first_base[0][max_index]]
            base = check_coverage(first_base, base, args.min_coverage,
                                        args.n_threshold,reference_pos[idx], reference=args.reference,
                                        reference_name=reference_name, start=start,
                                        end=end)
            cons_majority.extend(base)
        else: cons_majority.extend(['n']) # if we filtered out the whole column we use 'N'
        # add insertions
        if len(insert_matrix) > 0:
            inserts = collapse_insert_matrix(insert_matrix,
                                            min_coverage=args.min_coverage,
                                            min_freq= args.min_freq,
                                            authorised_alphabet=alphabet)
            cons_majority.extend(inserts)

        # 2.2 make consensus at column with deletions and without ambiguous bases
        index_deletions = np.in1d(first_base_ambiguous_deletions[0], alphabet_deletions)
        first_base_deletions = (first_base_ambiguous_deletions[0][index_deletions],
                                first_base_ambiguous_deletions[1][index_deletions])
        if first_base_deletions[0].size > 0: # i.e. if we havent filtered out the whole column
            # count if deletions account for more than all bases combined
            counts_del = first_base_deletions[1][first_base_deletions[0]=="*"]
            counts_notdel = np.sum(first_base_deletions[1][first_base_deletions[0]!="*"])
            # if deletions account for more than all bases combined output a deletion
            if counts_del.size > 0 and counts_del > counts_notdel:
                cons_majority_deletions.append("*")
            else:
                # set count of deletions to -1 so it will not be considered in the argmax
                first_base_deletions[1][first_base_deletions[0]=="*"] = -1
                # select most frequent base
                max_index_deletions = np.argmax(first_base_deletions[1])
                base = [first_base_deletions[0][max_index_deletions]]
                base = check_coverage(first_base_deletions,base,args.min_coverage,
                                      args.n_threshold,reference_pos[idx],
                                      reference=args.reference,reference_name=reference_name,
                                      start=start,end=end)
                cons_majority_deletions.extend(base)
        else: cons_majority.append('n') # if we filtered out the whole column we use 'N'
        # add insertions
        if len(insert_matrix) > 0:
            inserts = collapse_insert_matrix(insert_matrix,
                                            min_coverage=args.min_coverage,
                                            min_freq= args.min_freq,
                                            authorised_alphabet=alphabet)
            cons_majority_deletions.extend(inserts)

        # 2.3 make consensus at column without deletions and with ambiguous bases
        index_ambiguous = np.in1d(first_base_ambiguous_deletions[0], alphabet_ambiguous)
        first_base_ambiguous = (first_base_ambiguous_deletions[0][index_ambiguous],
                                first_base_ambiguous_deletions[1][index_ambiguous])

        if first_base_ambiguous[0].size > 0: # i.e. if we havent filtered out the whole column
            base = ambiguous_bases(first_base_ambiguous, args.min_freq)
            base = check_coverage(first_base_ambiguous, base, args.min_coverage,
                                    args.n_threshold,reference_pos[idx],
                                    reference=args.reference,reference_name=reference_name,
                                    start=start, end=end)
            cons_majority_ambiguous.extend(base)
        else: cons_majority.append('n') # if we filtered out the whole column we use 'N'
        # add insertions
        if len(insert_matrix) > 0:
            inserts = collapse_insert_matrix(insert_matrix,
                                            min_coverage=args.min_coverage,
                                            min_freq= args.min_freq,
                                            ambiguous=True,
                                            authorised_alphabet=alphabet_ambiguous)
            cons_majority_ambiguous.extend(inserts)

        # 2.4 make consensus at column with deletions and with ambiguous bases
        index_ambiguous_deletions = np.in1d(first_base_ambiguous_deletions[0], alphabet_ambiguous_deletions)
        first_base_ambiguous_deletions = (first_base_ambiguous_deletions[0][index_ambiguous_deletions],
                                          first_base_ambiguous_deletions[1][index_ambiguous_deletions])

        if first_base_ambiguous_deletions[0].size > 0: # i.e. if we havent filtered out the whole column
            # if deletion is majority print deletion
            # count if deletions account for more than all bases combined
            counts_del = first_base_ambiguous_deletions[1][first_base_ambiguous_deletions[0]=="*"]
            counts_notdel = np.sum(first_base_ambiguous_deletions[1][first_base_ambiguous_deletions[0]!="*"])
            # if deletions account for more than all bases combined output a deletion
            if counts_del.size > 0 and counts_del > counts_notdel:
                cons_majority_ambiguous_deletions.append("*")
            else:
                # set count of deletions to -1 so it will not be considered in the argmax
                first_base_ambiguous_deletions[1][first_base_ambiguous_deletions[0]=="*"] = -1
                # select most frequent base
                max_index_ambiguous_deletions = np.argmax(first_base_ambiguous_deletions[1])
                base = ambiguous_bases(first_base_ambiguous_deletions, args.min_freq)
                base = check_coverage(first_base_ambiguous_deletions, base, args.min_coverage,
                                       args.n_threshold,reference_pos[idx],
                                       reference=args.reference,reference_name=reference_name,
                                       start=start, end=end)
                cons_majority_ambiguous_deletions.extend(base)
        else: cons_majority.append('n') # if we filtered out the whole column we use 'N'
        # add insertions
        if len(insert_matrix) > 0:
            inserts = collapse_insert_matrix(insert_matrix,
                                            min_coverage=args.min_coverage,
                                            min_freq= args.min_freq,
                                            ambiguous=True,
                                            authorised_alphabet=alphabet_ambiguous)
            cons_majority_ambiguous.extend(inserts)

    # 3. Write to output

    # prepare and add leading and trailing n's for uncovered leading and trailing reference

    leading_n = reference_pos[0] * 'n'
    trailing_n = (reference_length - reference_pos[-1] -1) * 'n'

    cons_majority = leading_n + ''.join(cons_majority) + trailing_n
    cons_majority_deletions = leading_n + ''.join(cons_majority_deletions) + trailing_n
    cons_majority_ambiguous = leading_n + ''.join(cons_majority_ambiguous) + trailing_n
    cons_majority_ambiguous_deletions = leading_n + ''.join(cons_majority_ambiguous_deletions) + trailing_n

    # make seq records

    cons_majority = SeqRecord(
        Seq(cons_majority), id=sampleID,
        description="| Majority-vote rule")

    cons_majority_dels = SeqRecord(
        Seq(cons_majority_deletions), id=sampleID,
        description="| Majority-vote rule")

    cons_ambig = SeqRecord(
        Seq(cons_majority_ambiguous), id=sampleID,
        description="| Ambiguous bases")

    cons_ambig_dels = SeqRecord(
        Seq(cons_majority_ambiguous_deletions), id=sampleID,
        description="| Ambiguous bases")

    # write to output

    with open(os.path.join(args.outdir, "ref_majority.fasta"), "w") as outfile:
        SeqIO.write(cons_majority, outfile, "fasta")

    with open(os.path.join(args.outdir, "ref_ambig.fasta"), "w") as outfile:
        SeqIO.write(cons_ambig, outfile, "fasta")

    with open(os.path.join(args.outdir, "ref_majority_dels.fasta"),
              "w") as outfile:
        SeqIO.write(cons_majority_dels, outfile, "fasta")

    with open(os.path.join(args.outdir, "ref_ambig_dels.fasta"),
              "w") as outfile:
        SeqIO.write(cons_ambig_dels, outfile, "fasta")


if __name__ == '__main__':
    main()
